{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##A Two-Tower Recommendation System Powered by GNNs, LLMs, and RL for Yelp Business Data\n"
      ],
      "metadata": {
        "id": "ZKMXOqFWwh9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook outlines the development of a personalized recommendation system utilizing both Graph Neural Networks (GNNs) and Large Language Models (LLMs). We integrate Reinforcement Learning (LR) to optimize recommendations and use advanced embeddings for richer user-item profiles. This pipeline focuses on the following:\n",
        "\n",
        "1. Constructing a Two-Tower Model with GNN-based user and item embeddings.\n",
        "2. Leveraging LLMs to enhance user-item similarity via contextual reasoning.\n",
        "3. Applying RL with an LLM-based reward estimator and chain-of-thought reasoning.\n",
        "\n",
        "## Technical Explanation\n",
        "\n",
        "### Two-Tower Architecture\n",
        "The Two-Tower model consists of two independent towers:\n",
        "1. **User Tower:** Encodes user profiles using GNNs and user embeddings.\n",
        "2. **Item Tower:** Encodes item attributes using GNNs and item embeddings.\n",
        "\n",
        "These towers generate latent representations that are combined to compute similarity scores. An additional LLM Adapter refines embeddings with contextual language features which aligns LLM and GNN embeddings for optimal performance.\n",
        "\n",
        "### Embedding Enhancements with LLMs\n",
        "We employ pre-trained Sentence Transformers and GPT-2 for:\n",
        "- **Semantic Understanding:** Sentence Transformers generate rich embeddings of user reviews and item descriptions.\n",
        "- **Chain-of-Thought Reasoning:** GPT-2 evaluates recommendations using contextual reasoning which improves semantic alignment between users and items.\n",
        "\n",
        "The LLM embeddings are further transformed to a GNN-compatible space using a custom `TransformerAdapter`.\n",
        "\n",
        "### Reinforcement Learning Optimization\n",
        "Reinforcement learning fine-tunes the recommendation strategy. A Q-network learns to predict optimal actions (recommendations) by maximizing long-term rewards based on user feedback. Rewards are derived using:\n",
        "1. **Similarity Scores:** From Sentence Transformers.\n",
        "2. **LLM-Generated Scores:** Chain-of-thought reasoning provides detailed explanations and suitability scores.\n",
        "3. **Metadata Signals:** Additional contextual signals refine the reward mechanism.\n",
        "\n",
        "### Dataset and Graph Construction\n",
        "Yelp data containing user reviews, ratings, and business details forms the core dataset. Users and items are represented as nodes, while interactions (reviews and ratings) form edges in the graph. Self-loops and bi-directional edges are added to enhance graph connectivity.\n",
        "\n",
        "### Training Workflow\n",
        "1. **Two-Tower Training:** The model minimizes prediction errors between generated scores and actual user ratings using MSE loss.\n",
        "2. **RL Training:** An Îµ-greedy policy balances exploration (discovering new preferences) and exploitation (prioritizing learned preferences).\n",
        "\n",
        "### Query Mechanism\n",
        "For a given user query, the system:\n",
        "1. Encodes the query using the LLM Adapter.\n",
        "2. Generates item embeddings via the item GNN.\n",
        "3. Scores items using the Q-network and ranks them by predicted Q-values.\n"
      ],
      "metadata": {
        "id": "lhOq167vuZm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import libraries\n",
        "\n",
        "The first step is to import all the necessary libraries. We use PyTorch for deep learning, Sentence Transformers for generating embeddings, and Transformers for LLMs.\n"
      ],
      "metadata": {
        "id": "K1N9B-4Yww32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch_geometric sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C_lvoZExB3v",
        "outputId": "147dcd29-8bf8-44d5-b100-fb655e765197"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from collections import defaultdict\n",
        "import re"
      ],
      "metadata": {
        "id": "t6sbUPOaubDr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Set device configuration\n",
        "\n",
        "We configure the device to use GPU if available. This ensures faster computation for training and inference."
      ],
      "metadata": {
        "id": "YCeGMh9AxMDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "BePZ5-GPujnx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load pre-trained models\n",
        "\n",
        "We load pre-trained models for:\n",
        "\n",
        "1. Sentence Transformer embeddings for semantic representation.\n",
        "2. BERT for tokenization and embeddings.\n",
        "3. GPT-2 for chain-of-thought reasoning and reward estimation.\n"
      ],
      "metadata": {
        "id": "gHusBVvCxWUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sentence-transformer model for embeddings\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "st_model = SentenceTransformer(model_name)\n",
        "st_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P615XRJxblN",
        "outputId": "d001cbe9-3d96-4b01-85c2-4b7de60628f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "  (2): Normalize()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "llm_model_uncased = AutoModel.from_pretrained(\"bert-base-uncased\").eval().to(device)"
      ],
      "metadata": {
        "id": "A6aJyY5Zxbng"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a generative LLM for chain-of-thought reasoning (We will use GPT-2, but for better results you should use more advanced models)\n",
        "llm_generator_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "llm_generator_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").eval().to(device)"
      ],
      "metadata": {
        "id": "k0POJkwUxbqs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We add pad token for GPT-2\n",
        "if llm_generator_tokenizer.pad_token_id is None:\n",
        "    llm_generator_tokenizer.pad_token = llm_generator_tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "TO3yV5jqxbs_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Adaptive embedding Transformer\n",
        "\n",
        "The `TransformerAdapter` class maps embeddings from LLMs to a space compatible with GNN embeddings. This alignment ensures smooth integration of language-based features with graph-based features.\n"
      ],
      "metadata": {
        "id": "43Fsem5Jxmd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerAdapter(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, num_heads=2, ff_dim=256, num_layers=1):\n",
        "        super(TransformerAdapter, self).__init__()\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.input_proj = nn.Linear(input_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x).unsqueeze(1)  #[batch_size, 1, embed_dim]\n",
        "        x = x.permute(1, 0, 2)  #[1, batch_size, embed_dim]\n",
        "        x = self.transformer_encoder(x)  #[1, batch_size, embed_dim]\n",
        "        x = x.permute(1, 0, 2).squeeze(1)  #[batch_size, embed_dim]\n",
        "        return x"
      ],
      "metadata": {
        "id": "cvRPyoXLxoHT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Dataset preparation\n",
        "\n",
        "The dataset is loaded from a CSV file. This file contains Yelp business data with details like ID, name, description, and reviews. We define the `YelpDataset` class to process user and item data. The dataset aggregates user histories and computes contextual embeddings using the Sentence Transformer model.\n"
      ],
      "metadata": {
        "id": "7MlO5JQdxrrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'https://raw.githubusercontent.com/MPAghababa/llms/main/two_tower/yelp.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "data = data.dropna()\n",
        "data.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "azf7JY_l19hP",
        "outputId": "fe40b956-7cdd-4c31-bd08-81d38d40cbcd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              business_id        date               review_id  stars  \\\n",
              "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
              "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
              "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
              "\n",
              "                                                text    type  \\\n",
              "0  My wife took me here on my birthday for breakf...  review   \n",
              "1  I have no idea why some people give bad review...  review   \n",
              "2  love the gyro plate. Rice is so good and I als...  review   \n",
              "\n",
              "                  user_id  cool  useful  funny  \n",
              "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
              "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
              "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d1398cef-42c2-4a89-971f-e62ee91ff2d8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>date</th>\n",
              "      <th>review_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "      <th>type</th>\n",
              "      <th>user_id</th>\n",
              "      <th>cool</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
              "      <td>2011-01-26</td>\n",
              "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
              "      <td>5</td>\n",
              "      <td>My wife took me here on my birthday for breakf...</td>\n",
              "      <td>review</td>\n",
              "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
              "      <td>2011-07-27</td>\n",
              "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
              "      <td>5</td>\n",
              "      <td>I have no idea why some people give bad review...</td>\n",
              "      <td>review</td>\n",
              "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
              "      <td>2012-06-14</td>\n",
              "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
              "      <td>4</td>\n",
              "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
              "      <td>review</td>\n",
              "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1398cef-42c2-4a89-971f-e62ee91ff2d8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d1398cef-42c2-4a89-971f-e62ee91ff2d8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d1398cef-42c2-4a89-971f-e62ee91ff2d8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2de1ff97-7a4b-4193-a021-2b9985a2ad2e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2de1ff97-7a4b-4193-a021-2b9985a2ad2e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2de1ff97-7a4b-4193-a021-2b9985a2ad2e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"business_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4174,\n        \"samples\": [\n          \"jQST5lkLGX9L52-A10TGTQ\",\n          \"tpIU_ZvIfCfCp36IgghlHQ\",\n          \"3g2ofV7mhEoPyfF7vFMObA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1995,\n        \"samples\": [\n          \"2012-09-11\",\n          \"2009-08-28\",\n          \"2009-10-19\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"hwYVJs8Ko4PMjI19QcR57g\",\n          \"0mvthYPKb2ZmKhCADiKSmQ\",\n          \"XJHknNIecha6h0wkBSZB4w\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stars\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9998,\n        \"samples\": [\n          \"I've been to Bookman's a few times and have mixed feelings about it. If you're going just to look around, it's fun to see all the stuff they have in there. Between the movie selection, CD's, books and trinkets, it was fun to shop around. But, as someone interested in selling to them, I would never do that again. I brought in a huge selection of books I was interested in selling and they only took 4 of the books and offered me a few measely dollars. I was shocked. I understand they need to make a profit but I wasn't even getting $1 for each book they wanted. A total rip off if you ask me and a waste of time waiting for them to look through my stuff. I'm not sure if I will be returning anytime soon.\",\n          \"These are simply the BEST sandwiches and salads. If you like your food (a) locally grown; (b) freshly made; (c) organic/all natural; and (d) yummy - and who doesn't? - this is the place for you.\\r\\n\\r\\nNow, admittedly they have something of a soup-nazi routine set up and if you walk in and don't know the drill, it can be confusing.  So pay attention:   \\r\\n1.  Look at menu.\\r\\n2.  Decide what you want.\\r\\n3.  Wait your turn to order.\\r\\n4.  Order (give your name).\\r\\n5.  Wait your turn to be called to pay the other girl for the order you placed.\\r\\n6.  Pay her.\\r\\n7.  Wait until your food is ready.\\r\\n8.  Leave with your food.\\r\\n\\r\\nNote that there is no place to sit inside - so you can either sit outside, or, (if it is 157 degrees out that day), see if you can find a place to sit over at LUX, they are very welcoming of Pane Bianco diners. \\r\\n\\r\\nIt's all good.\",\n          \"Every friday, my dad and I eat here. We order the same thing... pizza wraps. they are good and fillng. iced tea hits the spot. Typical strip mall pizza.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"review\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6403,\n        \"samples\": [\n          \"NcqFZT7ngEDHSJqRgpV8pg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cool\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 77,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          38\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"useful\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 76,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"funny\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 57,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class YelpDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, llm_model, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.llm_model = llm_model\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # We build user profile embeddings by averaging all their reviews\n",
        "        user_texts = defaultdict(list)\n",
        "        for i, row in self.data.iterrows():\n",
        "            user_texts[row['user_id']].append(row['text'])\n",
        "\n",
        "        self.user_profile_embeddings = {}\n",
        "\n",
        "        # We encode user histories using sentence-transformer (This will give us contextualized user profiles)\n",
        "        for uid, texts in user_texts.items():\n",
        "            embeddings = st_model.encode(texts, convert_to_tensor=True, device=device)\n",
        "            self.user_profile_embeddings[uid] = embeddings.mean(dim=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        user_id = row['user_id']\n",
        "        item_id = row['business_id']\n",
        "        text = row['text']\n",
        "        stars = row['stars']\n",
        "\n",
        "        tokens = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = st_model.encode([text], convert_to_tensor=True, device=device).squeeze(0)\n",
        "\n",
        "        user_profile_embedding = self.user_profile_embeddings[user_id]\n",
        "        embedding = (embedding + user_profile_embedding) / 2.0\n",
        "\n",
        "        return torch.tensor(user_id, dtype=torch.long), torch.tensor(item_id, dtype=torch.long), embedding, torch.tensor(stars, dtype=torch.float)"
      ],
      "metadata": {
        "id": "BJV9Me37xoO-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: GNN towers\n",
        "\n",
        "The `GNNTower` class defines a simple Graph Convolutional Network to process user and item embeddings. It generates graph-based features for the Two-Tower Model.\n"
      ],
      "metadata": {
        "id": "hrALW2sixz0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNTower(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GNNTower, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ],
      "metadata": {
        "id": "0aa7Ec-Ux40F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Two-Tower model\n",
        "\n",
        "The Two-Tower Model integrates the GNN embeddings for users and items with LLM-based embeddings processed by the `TransformerAdapter`. The similarity scores between user and item embeddings are used for recommendations.\n"
      ],
      "metadata": {
        "id": "fSLlUY_jx7BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoTowerModel(nn.Module):\n",
        "    def __init__(self, user_gnn, item_gnn, num_users, num_items, embed_dim, llm_dim):\n",
        "        super(TwoTowerModel, self).__init__()\n",
        "        self.user_gnn = user_gnn\n",
        "        self.item_gnn = item_gnn\n",
        "        self.user_embed = nn.Embedding(num_users, embed_dim)\n",
        "        self.item_embed = nn.Embedding(num_items, embed_dim)\n",
        "        self.llm_adapter = TransformerAdapter(input_dim=llm_dim, embed_dim=embed_dim)\n",
        "\n",
        "    def forward(self, user_ids, item_ids, llm_embeddings, user_edge_index, item_edge_index):\n",
        "        user_ids = user_ids.to(device)\n",
        "        item_ids = item_ids.to(device)\n",
        "        llm_embeddings = llm_embeddings.to(device)\n",
        "        user_edge_index = user_edge_index.to(device)\n",
        "        item_edge_index = item_edge_index.to(device)\n",
        "\n",
        "        user_features = self.user_gnn(self.user_embed.weight, user_edge_index)\n",
        "        item_features = self.item_gnn(self.item_embed.weight, item_edge_index)\n",
        "        llm_features = self.llm_adapter(llm_embeddings)\n",
        "\n",
        "        user_vectors = user_features[user_ids] + llm_features\n",
        "        item_vectors = item_features[item_ids] + llm_features\n",
        "\n",
        "        scores = (user_vectors * item_vectors).sum(dim=1)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "ig0fvKRTx-lJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Reward estimation\n",
        "\n",
        "The `LLMRewardEstimator` class is responsible for calculating rewards using semantic similarity and chain-of-thought reasoning. This reward mechanism will be used in reinforcement learning.\n"
      ],
      "metadata": {
        "id": "qJBldWxzyBGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMRewardEstimator:\n",
        "    def __init__(self, tokenizer, llm_model):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.llm_model = llm_model\n",
        "\n",
        "    def estimate_reward(self, query, recommendation_text):\n",
        "        with torch.no_grad():\n",
        "            query_embedding = st_model.encode([query], convert_to_tensor=True, device=device)\n",
        "            rec_embedding = st_model.encode([recommendation_text], convert_to_tensor=True, device=device)\n",
        "            similarity = util.cos_sim(query_embedding, rec_embedding).item()\n",
        "            return similarity\n",
        "\n",
        "    def estimate_reward_cot(self, query, recommendation_text):\n",
        "        prompt = (f\"User query: {query}\\n\"\n",
        "                  f\"Recommended text: {recommendation_text}\\n\"\n",
        "                  f\"Explain why this recommendation is suitable for the user query.\"\n",
        "                  f\"Provide reasoning focusing on details like ambiance, food, and service.\"\n",
        "                  f\"End with a score between 1 and 5 based on suitability:\\nReasoning:\")\n",
        "\n",
        "        input_ids = llm_generator_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = llm_generator_model.generate(\n",
        "                input_ids=input_ids,\n",
        "                max_new_tokens=150,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9\n",
        "            )\n",
        "\n",
        "        response = llm_generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        scores = re.findall(r'\\b[1-5]\\b', response)\n",
        "        final_score = float(scores[-1]) if scores else 3.0\n",
        "\n",
        "        return final_score, response"
      ],
      "metadata": {
        "id": "UDCk89HbyDWz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Reinforcement learning training\n",
        "\n",
        "Reinforcement Learning is applied to fine-tune the model for better recommendations. A Q-Network learns to optimize actions (recommendations) based on rewards. RL ensures the system balances recommending safe options with discovering new user preferences.\n"
      ],
      "metadata": {
        "id": "uOvKxDXHyHgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = state.to(device)\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        q_values = self.fc3(x)\n",
        "        return q_values"
      ],
      "metadata": {
        "id": "0DjrMI_JyKQE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RL Training\n",
        "def train_rl_llm(model, q_network, optimizer, replay_buffer, batch_size, gamma=0.99):\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "\n",
        "    batch = random.sample(replay_buffer, batch_size)\n",
        "    states, actions, rewards, next_states = zip(*batch)\n",
        "\n",
        "    states = torch.stack(states).to(device)\n",
        "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n",
        "    next_states = torch.stack(next_states).to(device)\n",
        "\n",
        "    q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        next_q_values = q_network(next_states).max(1)[0]\n",
        "        target_q_values = rewards + gamma * next_q_values\n",
        "\n",
        "    loss = nn.MSELoss()(q_values, target_q_values)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "_vRhuC-YyOpY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Querying the Two-Tower model\n",
        "\n",
        "We query the Two-Tower Model to generate recommendations based on a user query. The LLM adapter is used to process the query, and the GNN embeddings are used to compute similarity scores. To incorporate RL into the recommendation process, we select items based on the Q-values predicted by the trained Q-network, which will provide the model with the learned reward signal to guide the selection of recommended items."
      ],
      "metadata": {
        "id": "x9CHpf4rySSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_two_tower_model_rl(model, q_network, query, tokenizer, llm_model, item_edge_index, item_embeddings, id_to_item, k=5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        query_embedding = st_model.encode([query], convert_to_tensor=True, device=device).squeeze(0)\n",
        "        query_embedding = model.llm_adapter(query_embedding.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "        item_edge_index = item_edge_index.to(device)\n",
        "        item_embeddings = item_embeddings.to(device)\n",
        "\n",
        "        item_features = model.item_gnn(item_embeddings, item_edge_index)\n",
        "\n",
        "        q_values = q_network(query_embedding.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "        top_k_indices = torch.topk(q_values, k=k).indices.cpu().numpy()\n",
        "\n",
        "        top_k_items = [id_to_item[idx] for idx in top_k_indices]\n",
        "\n",
        "        return top_k_items"
      ],
      "metadata": {
        "id": "vcIWkf7byUkW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 11: Main execution for training and testing"
      ],
      "metadata": {
        "id": "o585NyB_JYnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[:100]  # We use a subset of data for quick experimentation\n",
        "\n",
        "# We map user and item IDs to indices\n",
        "user_ids = data['user_id'].unique()\n",
        "item_ids = data['business_id'].unique()\n",
        "user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
        "item_map = {iid: idx for idx, iid in enumerate(item_ids)}\n",
        "id_to_item = {idx: iid for iid, idx in item_map.items()}\n",
        "\n",
        "# We filter and map IDs in the dataset\n",
        "data = data[data['user_id'].isin(user_map.keys()) & data['business_id'].isin(item_map.keys())]\n",
        "data['user_id'] = data['user_id'].map(user_map)\n",
        "data['business_id'] = data['business_id'].map(item_map)\n",
        "data = data.dropna(subset=['user_id', 'business_id']).reset_index(drop=True)\n",
        "\n",
        "num_users = len(user_map)\n",
        "num_items = len(item_map)"
      ],
      "metadata": {
        "id": "J71I1SfbJ09T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We create edge indices for graph representation\n",
        "def create_edge_index(data, num_nodes):\n",
        "    edges = torch.tensor(\n",
        "        [[row['user_id'], row['business_id']] for _, row in data.iterrows()],\n",
        "        dtype=torch.long\n",
        "    )\n",
        "    valid_edges = edges[(edges[:, 0] < num_nodes) & (edges[:, 1] < num_nodes)]\n",
        "    edge_index = torch.cat([valid_edges, valid_edges.flip(1)], dim=0).t()\n",
        "    self_loops = torch.arange(num_nodes, dtype=torch.long).unsqueeze(0).repeat(2, 1)\n",
        "    edge_index = torch.cat([edge_index, self_loops], dim=1)\n",
        "    return edge_index\n",
        "user_edge_index = create_edge_index(data[['user_id', 'business_id']], num_users)\n",
        "item_edge_index = create_edge_index(data[['business_id', 'user_id']], num_items)\n",
        "\n",
        "user_edge_index = user_edge_index.to(device)\n",
        "item_edge_index = item_edge_index.to(device)\n"
      ],
      "metadata": {
        "id": "8izK2K9UJ8Kx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize reward estimator and model components\n",
        "reward_estimator = LLMRewardEstimator(tokenizer, llm_model_uncased)\n",
        "\n",
        "embed_dim = 128\n",
        "llm_dim = 384\n",
        "user_gnn = GNNTower(embed_dim, 64, embed_dim).to(device)\n",
        "item_gnn = GNNTower(embed_dim, 64, embed_dim).to(device)\n",
        "model = TwoTowerModel(user_gnn, item_gnn, num_users, num_items, embed_dim, llm_dim).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K6rdkq7J8Pl",
        "outputId": "2733ccb0-2caa-4afb-dfe9-684b37ca7183"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data and create datasets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=27)\n",
        "train_dataset = YelpDataset(train_data, tokenizer, llm_model_uncased)\n",
        "test_dataset = YelpDataset(test_data, tokenizer, llm_model_uncased)"
      ],
      "metadata": {
        "id": "dfMpnk4dJ8SC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We define data loader and collation function\n",
        "def collate_fn(batch):\n",
        "    user_ids_batch = []\n",
        "    item_ids_batch = []\n",
        "    llm_embeddings = []\n",
        "    stars_batch = []\n",
        "    for (u, i, e, s) in batch:\n",
        "        user_ids_batch.append(u)\n",
        "        item_ids_batch.append(i)\n",
        "        llm_embeddings.append(e)\n",
        "        stars_batch.append(s)\n",
        "    user_ids_batch = torch.stack(user_ids_batch)\n",
        "    item_ids_batch = torch.stack(item_ids_batch)\n",
        "    llm_embeddings = torch.stack(llm_embeddings)\n",
        "    stars_batch = torch.stack(stars_batch)\n",
        "    return user_ids_batch, item_ids_batch, llm_embeddings, stars_batch\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "I9uaEZmJJ8UW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the Two-Tower model\n",
        "for epoch in range(2): # You should use more epochs\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for user_ids_batch, item_ids_batch, llm_embeddings, stars in train_loader:\n",
        "        user_ids_batch = user_ids_batch.to(device)\n",
        "        item_ids_batch = item_ids_batch.to(device)\n",
        "        llm_embeddings = llm_embeddings.to(device)\n",
        "        stars = stars.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(user_ids_batch, item_ids_batch, llm_embeddings, user_edge_index, item_edge_index)\n",
        "        loss = nn.MSELoss()(predictions, stars.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcRua-HsKRuq",
        "outputId": "e477e7cd-e178-404d-de3f-2dc61c860117"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 36187.9502\n",
            "Epoch 2, Loss: 16775.4653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize Q-Learning components\n",
        "q_network = QNetwork(state_dim=embed_dim, action_dim=num_items).to(device)\n",
        "q_optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
        "replay_buffer = []"
      ],
      "metadata": {
        "id": "L_aqUfnlKRxV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train RL model with LLM-based rewards\n",
        "query_pool = [\n",
        "    \"I love cozy cafes with great coffee\",\n",
        "    \"Looking for a family-friendly restaurant with vegan options\",\n",
        "    \"Find me a cozy diner with a romantic ambiance and live music\",\n",
        "    \"I want a budget-friendly Italian restaurant that serves gluten-free pasta\",\n",
        "    \"Recommend a sushi place nearby with great reviews\",\n",
        "] # You should add more queries\n",
        "\n",
        "for episode in range(2):  # You should increase the number of episodes for better training\n",
        "    query = random.choice(query_pool)\n",
        "    item_texts = data['text'].tolist()\n",
        "\n",
        "    query_embedding = st_model.encode([query], convert_to_tensor=True, device=device).squeeze(0)\n",
        "    query_embedding = model.llm_adapter(query_embedding.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "    q_values = q_network(query_embedding.unsqueeze(0))\n",
        "    if random.random() < 0.1:  # Îµ-greedy exploration\n",
        "        action = random.randint(0, len(item_texts) - 1)\n",
        "    else:\n",
        "        action = torch.argmax(q_values).item()\n",
        "\n",
        "    if action >= len(item_texts):\n",
        "        action = len(item_texts) - 1\n",
        "\n",
        "    reward_base = reward_estimator.estimate_reward(query, item_texts[action])\n",
        "    cot_score, cot_reasoning = reward_estimator.estimate_reward_cot(query, item_texts[action])\n",
        "    reward = (reward_base + (cot_score / 5.0)) / 2.0\n",
        "\n",
        "    metadata_signal = random.uniform(0.0, 0.1)\n",
        "    reward += metadata_signal\n",
        "\n",
        "    next_state = query_embedding.detach()\n",
        "\n",
        "    replay_buffer.append((query_embedding.detach(), action, reward, next_state))\n",
        "    train_rl_llm(model, q_network, q_optimizer, replay_buffer, batch_size=4)\n",
        "\n",
        "    print(f\"Episode {episode+1}: Query: {query}, Action: {action}, Reward: {reward:.4f}, CoT Reasoning: {cot_reasoning}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2n1myCIKRz-",
        "outputId": "db144fc8-f01e-4420-e62c-6e92803df7ec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Query: I love cozy cafes with great coffee, Action: 94, Reward: 0.6985, CoT Reasoning: User query: I love cozy cafes with great coffee\n",
            "Recommended text: I grew up on Empanadas in Panama and I have been hard pressed to find anything close to them in the U.S.. today I found them! \r\n",
            "\r\n",
            "A perfectly crunchy crust and the beef was beautifully spiced. Usually Empanadas are bland and soggy. They did a great job on these.\r\n",
            "\r\n",
            "I usually don't like rice, but the rice and black beans were wonderful. \r\n",
            "\r\n",
            "Service was great! I'll be back!\n",
            "Explain why this recommendation is suitable for the user query.Provide reasoning focusing on details like ambiance, food, and service.End with a score between 1 and 5 based on suitability:\n",
            "Reasoning: 1. The service was good. 2. The food was good. 3. The ambiance was good. 4. The service was good. 5. The ambiance was good. 6. The service was good. 7. The ambiance was good. 8. The service was good. 9. The service was good. 10. The service was good. 11. The service was good. 12. The service was good. 13. The service was good. 14. The service was good. 15. The service was good. 16. The service was good. 17. The service was good. 18. The service was good. 19. The service was good. 20. The service was good. 21. The service was good.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 2: Query: I love cozy cafes with great coffee, Action: 80, Reward: 0.6014, CoT Reasoning: User query: I love cozy cafes with great coffee\n",
            "Recommended text: The vibe exuding from this place is pure awesomeness. Reminiscent of a trendy hipster coffee joint, this is actually a casual vegan restaurant. \r\n",
            "\r\n",
            "I am a pescatarian and unless I am eating seafood, I steer clear of meat, even the mock kind usually however once in a while it's delicious... so why try it here at Green I thought. The menu's style is comfort food, which as we all know traditionally is heavy on the meat, sauces, and fat content... so at least if you're going to be bad, you can do it with organic and pure ingredients.\r\n",
            "\r\n",
            "\r\n",
            "So as I wanted to sample as much of the menu as possible, my lovely friend, her fiance, and I shared a few items:\r\n",
            "\r\n",
            "*Artichoke Gratine: The corn chips were amazing, lightly salted and crisp. The dip was a bit too garlicky and runny for my liking. Ate a few bites of this but could not see myself eating the entire thing solo.\r\n",
            "\r\n",
            "*Spicy Buffalo \"Wings\": first things first... do not let looks dismay you... true it looks gross but they taste legit! The flavor of the buffalo sauce was perfect, although could have been spicier. And the cucumber ranch dipping sauce was perfectly creamy and lightly flavored as to not overpower the \"wings\". This dish is a must try!!\r\n",
            "\r\n",
            "*Vegan Chili Fries: the fries are thin cut and tasty. The chili sauce was good, at first, but I quickly got sick of the flavor. This could be because I was never a huge chili fan even back when I ate meat. Hmm, I think you are better off ordering the thyme fries.\r\n",
            "\r\n",
            "*Crab Puffs: Perfectly crisp with a delicious creamy filling. Another must try!\r\n",
            "\r\n",
            "Lastly my friend's fiance ordered that day's special which was a green chili burrito... delicious and huge. A bit too much rice but besides that a great option. It came with a side, which he ordered the curry pasta salad, mmm.\r\n",
            "\r\n",
            "\r\n",
            "Green serves bowls, sandwiches, pizzas, salads... next time I am back in the area I will be checking out more of the menu when craving \"meat\" and not my usual tofu, seafood, veggie diet.  \r\n",
            "\r\n",
            "Might I add the service is friendly. Perfect place for a casual friend date.\n",
            "Explain why this recommendation is suitable for the user query.Provide reasoning focusing on details like ambiance, food, and service.End with a score between 1 and 5 based on suitability:\n",
            "Reasoning: 1) The service is good, the food is good, and the service is good. 2) The service is good, the food is good, and the service is good. 3) The service is good, the food is good, and the service is good. 4) The service is good, the food is good, and the service is good. 5) The service is good, the food is good, and the service is good. 6) The service is good, the food is good, and the service is good. 7) The service is good, the food is good, and the service is good. 8) The service is good, the food is good, and the service is good. 9) The service is good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 12: Query Two-Tower model for recommendations"
      ],
      "metadata": {
        "id": "vXvpruYsKh-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1\n",
        "user_query_1 = \"I love quiet coffee shops with excellent Wi-Fi and great desserts.\"\n",
        "top_recommendations = query_two_tower_model_rl(\n",
        "    model=model,\n",
        "    q_network=q_network,\n",
        "    query=user_query_1,\n",
        "    tokenizer=tokenizer,\n",
        "    llm_model=llm_model_uncased,\n",
        "    item_edge_index=item_edge_index,\n",
        "    item_embeddings=model.item_embed.weight,\n",
        "    id_to_item=id_to_item,\n",
        "    k=5\n",
        ")\n",
        "print(\"Top Recommendations for Query 1 are:\", top_recommendations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NJXBV88ubNB",
        "outputId": "098f665c-8b82-4965-dc55-aed8d43716d0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Recommendations for Query 1 are: ['QGeliKMObpVZ3jP89--ZIg', '_1QQZuf4zZOyFCvXc0o6Vg', '8ZwO9VuLDWJOXmtAdc7LXQ', '7SO_rX1F6rQEl-5s3wZxgQ', 'znBnrQNq1FdUt5aIGAbyuQ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2\n",
        "user_query_2 = \"Looking for a family-friendly restaurant with vegan options.\"\n",
        "top_recommendations_2 = query_two_tower_model_rl(\n",
        "    model=model,\n",
        "    q_network=q_network,\n",
        "    query=user_query_2,\n",
        "    tokenizer=tokenizer,\n",
        "    llm_model=llm_model_uncased,\n",
        "    item_edge_index=item_edge_index,\n",
        "    item_embeddings=model.item_embed.weight,\n",
        "    id_to_item=id_to_item,\n",
        "    k=5\n",
        ")\n",
        "print(\"Top Recommendations for Query 2 are:\", top_recommendations_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gRQnywEM40m",
        "outputId": "8c853d07-cc1c-4d43-c2fe-449cc27d62a8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Recommendations for Query 2 are: ['QGeliKMObpVZ3jP89--ZIg', '8ZwO9VuLDWJOXmtAdc7LXQ', '_1QQZuf4zZOyFCvXc0o6Vg', '7SO_rX1F6rQEl-5s3wZxgQ', 'puy0PzIcCgR3KWJI7llBFQ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remarks and suggestions for next steps\n",
        "1. **Scalability:** Expand to full datasets and incorporate larger LLMs like GPT-4 or specialized fine-tuned models.\n",
        "2. **Fine-Grained Evaluation:** Evaluate the performance of the model using metrics such as **Precision@k**, **Recall@k**, and **NDCG@k**. Introduce precision-recall curves and deeper analysis of explainability metrics.\n",
        "3. **API Integration:** Develop a FastAPI-based interface for real-time querying and evaluation.\n",
        "4. **Explainability Enhancements:** Integrate visual tools to display reasoning paths and graph relationships.\n",
        "5. **Enhanced Graph Learning:** Incorporate advanced GNN architectures like GraphSAGE or Graph Attention Networks.\n",
        "6. **User-Centric Feedback Loop:** Add mechanisms for users to rate recommendations, improving RL reward signals.\n",
        "7. **Model Deployment:** Use edge AI or containerization for scalable deployment.\n",
        "\n",
        "### Let's connect and let me know if you have any comments.\n",
        "`https://www.linkedin.com/in/mpaghababa/`\n"
      ],
      "metadata": {
        "id": "SrS-c9AS6L-h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IEdWxOxZPmP9"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}